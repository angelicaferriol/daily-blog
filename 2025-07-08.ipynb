{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f533d7a2",
   "metadata": {},
   "source": [
    "# Daily Blog #69 - Dimensionality Reduction with PCA (Principal Component Analysis)\n",
    "### July 8, 2025 \n",
    "\n",
    "When working with large datasets, especially ones with many features (columns), models can:\n",
    "\n",
    "* Overfit easily (too much noise)\n",
    "* Run slower\n",
    "* Become hard to interpret\n",
    "\n",
    "**PCA** helps reduce the number of features while keeping as much **variance** (information) as possible.\n",
    "\n",
    "\n",
    "### **Core Idea**\n",
    "\n",
    "PCA transforms the original features into **new axes** (called *principal components*) that are:\n",
    "\n",
    "* **Uncorrelated**\n",
    "* Ordered so that the **first few capture most of the variance**\n",
    "\n",
    "Imagine rotating the entire dataset so that most of the meaningful information aligns with the first axis.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose you have 3 features: height, weight, and BMI. They're highly correlated.\n",
    "\n",
    "PCA can reduce this to just **1 or 2 principal components** that still retain the essence of the data.\n",
    "\n",
    "\n",
    "### Step-by-Step Breakdown:\n",
    "\n",
    "#### 1. **Standardize** the Data\n",
    "\n",
    "PCA is sensitive to scale. Always normalize:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "#### 2. **Apply PCA**\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "```\n",
    "\n",
    "#### 3. **Inspect Explained Variance**\n",
    "\n",
    "```python\n",
    "print(pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "This tells how much variance each component holds.\n",
    "\n",
    "\n",
    "### Visualization\n",
    "\n",
    "You can now plot your 2D PCA-reduced data:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Result\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Takeaways:\n",
    "\n",
    "* PCA is **unsupervised** – doesn’t care about labels.\n",
    "* Great for **visualizing** high-dimensional data.\n",
    "* Use it **before clustering or classification** to improve speed and performance.\n",
    "* But: You lose **interpretability** – PCs are combinations of original features.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

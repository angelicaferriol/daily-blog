{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "861a14b1",
   "metadata": {},
   "source": [
    "# Daily Blog #90 - Data Science (Full) Overview\n",
    "### July 29, 2025 \n",
    "\n",
    "---\n",
    "\n",
    "### 1. What is Data Science?\n",
    "\n",
    "Data Science is an interdisciplinary field that utilizes scientific methods, algorithms, and systems to extract insights and knowledge from structured and unstructured data. It integrates principles from statistics, computer science, mathematics, and domain expertise to process, analyze, and interpret large volumes of data. The primary goal is to support data-driven decision-making by uncovering patterns, predicting outcomes, and enabling automation through machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. NumPy & Pandas Quick Crash\n",
    "\n",
    "**NumPy** (Numerical Python) is a foundational Python library for numerical computing. It provides high-performance, multidimensional array objects and tools for performing operations such as linear algebra, Fourier transforms, and random number generation. Arrays in NumPy are more efficient than native Python lists due to vectorized operations and memory optimization.\n",
    "\n",
    "**Pandas** is a powerful data manipulation and analysis library built on top of NumPy. It introduces two core data structures: `Series` (one-dimensional) and `DataFrame` (two-dimensional). Pandas simplifies data cleaning, filtering, reshaping, joining datasets, and time series analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Data Cleaning\n",
    "\n",
    "Data cleaning is the process of detecting and correcting errors or inconsistencies in datasets to improve data quality. This includes handling missing values, removing duplicates, correcting data types, fixing structural errors, and filtering outliers. Effective data cleaning ensures the accuracy and reliability of downstream analysis and modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Data Wrangling\n",
    "\n",
    "Data wrangling, also known as data munging, involves transforming raw data into a structured and usable format. This may include merging multiple datasets, reshaping data (e.g., pivoting or melting), aggregating values, parsing strings, and converting formats. It is a critical step that prepares data for analysis and modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA is the initial phase of data analysis that focuses on summarizing the main characteristics of a dataset, often through visualizations and descriptive statistics. The objective is to uncover underlying structures, detect outliers, identify patterns, test assumptions, and guide further analysis. EDA serves as a bridge between raw data and formal modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Visualization (Matplotlib & Seaborn)\n",
    "\n",
    "Data visualization enables the graphical representation of data to identify patterns, trends, and outliers. **Matplotlib** is a fundamental plotting library in Python that supports line plots, bar charts, scatter plots, histograms, and more. **Seaborn**, built on top of Matplotlib, offers a higher-level interface with built-in themes and statistical plots such as boxplots, violin plots, and heatmaps. Both libraries are essential tools in EDA and model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Descriptive Statistics\n",
    "\n",
    "Descriptive statistics summarize and describe the basic features of a dataset. Key measures include:\n",
    "\n",
    "* **Central tendency**: Mean, median, and mode\n",
    "* **Dispersion**: Range, variance, and standard deviation\n",
    "* **Shape**: Skewness and kurtosis\n",
    "\n",
    "These statistics help understand the distribution and variability of data before applying inferential techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Probability Basics\n",
    "\n",
    "Probability is the branch of mathematics concerned with quantifying uncertainty. Basic concepts include:\n",
    "\n",
    "* **Experiment**: An action with uncertain outcomes\n",
    "* **Sample space**: The set of all possible outcomes\n",
    "* **Event**: A subset of outcomes\n",
    "* **Probability of an event**: A value between 0 and 1 indicating the likelihood of occurrence\n",
    "\n",
    "Foundational rules include addition and multiplication rules, conditional probability, and Bayes’ theorem.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Distributions\n",
    "\n",
    "A probability distribution describes how the values of a random variable are distributed. Common distributions include:\n",
    "\n",
    "* **Normal distribution**: Bell-shaped and symmetric\n",
    "* **Binomial distribution**: Describes the number of successes in a fixed number of independent trials\n",
    "* **Poisson distribution**: Models the number of events in a fixed interval\n",
    "* **Uniform distribution**: All outcomes are equally likely\n",
    "\n",
    "Understanding distributions is crucial for hypothesis testing and model assumptions.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Hypothesis Testing\n",
    "\n",
    "Hypothesis testing is a statistical method used to make inferences about a population based on sample data. It involves:\n",
    "\n",
    "* **Null hypothesis (H₀)**: Assumes no effect or difference\n",
    "* **Alternative hypothesis (H₁)**: Assumes an effect or difference exists\n",
    "* **Test statistic**: A value calculated from the data\n",
    "* **p-value**: The probability of obtaining a result at least as extreme as the observed, under H₀\n",
    "* **Significance level (α)**: The threshold to reject H₀, commonly 0.05\n",
    "\n",
    "Tests include t-tests, chi-square tests, and ANOVA, depending on the data type and context.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Correlation vs. Causation\n",
    "\n",
    "**Correlation** measures the strength and direction of a linear relationship between two variables but does not imply causality. **Causation** implies that one variable directly affects another. Misinterpreting correlation as causation can lead to false conclusions. Establishing causality requires controlled experiments or strong statistical inference methods like causal graphs or instrumental variables.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Linear Regression\n",
    "\n",
    "Linear regression models the relationship between a dependent variable and one or more independent variables by fitting a straight line. The basic form is:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1x + \\varepsilon\n",
    "$$\n",
    "\n",
    "Where $y$ is the target variable, $x$ is the predictor, $\\beta$s are coefficients, and $\\varepsilon$ is the error term. It assumes linearity, homoscedasticity, independence, and normality of residuals.\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Logistic Regression\n",
    "\n",
    "Logistic regression is used when the dependent variable is categorical, typically binary. It models the probability of an event occurring using the logistic function:\n",
    "\n",
    "$$\n",
    "P(y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}\n",
    "$$\n",
    "\n",
    "It estimates odds ratios and is widely used in classification problems such as spam detection or disease diagnosis.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Decision Trees\n",
    "\n",
    "Decision trees are non-parametric models that predict a target variable by learning simple decision rules from data features. Each internal node represents a test on a feature, each branch represents an outcome, and each leaf node represents a prediction. Trees can handle both classification and regression tasks. They are interpretable but prone to overfitting, which can be mitigated by pruning or ensemble methods like Random Forests.\n",
    "\n",
    "---\n",
    "\n",
    "### 15. Clustering (K-means)\n",
    "\n",
    "Clustering is an unsupervised learning technique that groups similar data points together. **K-means** partitions data into *k* clusters by minimizing the within-cluster variance. The algorithm involves:\n",
    "\n",
    "1. Initializing *k* centroids\n",
    "2. Assigning each point to the nearest centroid\n",
    "3. Updating centroids based on cluster members\n",
    "4. Repeating until convergence\n",
    "\n",
    "It assumes spherical clusters and is sensitive to initial centroids and outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### 16. Dimensionality Reduction (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique to reduce the number of variables in a dataset while retaining most of the variance. It transforms the original features into a new set of orthogonal variables (principal components), ordered by explained variance. PCA is useful for visualization, noise reduction, and speeding up algorithms, but it may reduce interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "### 17. Data Ethics & Bias\n",
    "\n",
    "Data ethics concerns the responsible collection, analysis, and use of data. Key principles include:\n",
    "\n",
    "* **Privacy**: Respecting individuals' data rights\n",
    "* **Transparency**: Disclosing how data is used and models are built\n",
    "* **Accountability**: Holding systems and creators responsible for outcomes\n",
    "* **Fairness**: Avoiding algorithmic bias and discrimination\n",
    "\n",
    "Bias in data or models can arise from historical inequalities, flawed sampling, or improper feature selection. Ethical data science requires constant vigilance, stakeholder input, and alignment with societal values.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
